<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Quantization on Yida Wang</title>
    <link>https://wangyida.github.io/tags/quantization/</link>
    <description>Recent content in Quantization on Yida Wang</description>
    <image>
      <title>Yida Wang</title>
      <url>http://campar.in.tum.de/twiki/pub/Main/YidaWang/YidaWang_CAMP.png</url>
      <link>http://campar.in.tum.de/twiki/pub/Main/YidaWang/YidaWang_CAMP.png</link>
    </image>
    <generator>Hugo -- 0.147.8</generator>
    <language>en</language>
    <lastBuildDate>Thu, 01 Sep 2016 10:15:01 +0200</lastBuildDate>
    <atom:link href="https://wangyida.github.io/tags/quantization/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Efficient deep learning for real object recognition based on 3D models</title>
      <link>https://wangyida.github.io/posts/zigzagnet/</link>
      <pubDate>Thu, 01 Sep 2016 10:15:01 +0200</pubDate>
      <guid>https://wangyida.github.io/posts/zigzagnet/</guid>
      <description>&lt;h1 id=&#34;abstrarct&#34;&gt;Abstrarct&lt;/h1&gt;
&lt;p&gt;Effective utilization on texture-less 3D models for deep learning is significant to recognition on real photos. We eliminate the reliance on massive real training data by modifying convolutional neural network in 3 aspects: synthetic data rendering for training data generation in large quantities, multi-triplet cost function modification for multi-task learning and compact micro architecture design for producing tiny parametric model while overcoming over-fit problem in texture-less models. Network is initiated with multi-triplet cost function establishing sphere-like distribution of descriptors in each category which is helpful for recognition on regular photos according to pose, lighting condition, background and category information of rendered images. Fine-tuning with additional data further meets the aim of classification on special real photos based on initial model. We propose a 6.2 MB compact parametric model called ZigzagNet based on SqueezeNet to improve the performance for recognition by applying moving normalization inside micro architecture and adding channel wise convolutional bypass through macro architecture.  Moving batch normalization is used to get a good performance on both convergence speed and recognition accuracy. Accuracy of our compact parametric model in experiment on ImageNet and PASCAL samples provided by PASCAL3D+ based on simple Nearest Neighbor classifier is close to the result of 240 MB AlexNet trained with real images. Model trained on texture-less models which consumes less time for rendering and collecting outperforms the result of training with more textured models from ShapeNet.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
