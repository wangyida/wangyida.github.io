<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>RAL on Yida Wang</title>
    <link>https://wangyida.github.io/tags/ral/</link>
    <description>Recent content in RAL on Yida Wang</description>
    <image>
      <url>http://campar.in.tum.de/twiki/pub/Main/YidaWang/YidaWang_CAMP.png</url>
      <link>http://campar.in.tum.de/twiki/pub/Main/YidaWang/YidaWang_CAMP.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sat, 01 Jun 2019 10:15:01 +0200</lastBuildDate><atom:link href="https://wangyida.github.io/tags/ral/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Variational Object-aware 3D Hand Pose from a Single RGB Image</title>
      <link>https://wangyida.github.io/posts/2019_1/youtube/</link>
      <pubDate>Sat, 01 Jun 2019 10:15:01 +0200</pubDate>
      
      <guid>https://wangyida.github.io/posts/2019_1/youtube/</guid>
      <description>Re-direct to the full PAPER and CODE |
   Abstrarct We propose an approach to estimate the 3D pose of a human hand while grasping objects from a single RGB image. Our approach is based on a probabilistic model implemented with deep architectures, which is used for regressing, respectively, the 2D hand joints heat maps and the 3D hand joints coordinates. We train our networks so to make our approach robust to large object- and self-occlusions, as commonly occurring with the task at hand.</description>
    </item>
    
  </channel>
</rss>
