<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>IJCV on Yida Wang</title>
    <link>https://wangyida.github.io/tags/ijcv/</link>
    <description>Recent content in IJCV on Yida Wang</description>
    <image>
      <title>Yida Wang</title>
      <url>http://campar.in.tum.de/twiki/pub/Main/YidaWang/YidaWang_CAMP.png</url>
      <link>http://campar.in.tum.de/twiki/pub/Main/YidaWang/YidaWang_CAMP.png</link>
    </image>
    <generator>Hugo -- 0.147.6</generator>
    <language>en</language>
    <lastBuildDate>Wed, 29 Dec 2021 10:15:01 +0200</lastBuildDate>
    <atom:link href="https://wangyida.github.io/tags/ijcv/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>SoftPool&#43;&#43; / An Encoder-Decoder Network for Point Cloud Completion</title>
      <link>https://wangyida.github.io/posts/ijcv22/</link>
      <pubDate>Wed, 29 Dec 2021 10:15:01 +0200</pubDate>
      <guid>https://wangyida.github.io/posts/ijcv22/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Re-direct to the full &lt;a href=&#34;https://link.springer.com/article/10.1007/s11263-022-01588-7&#34;&gt;&lt;strong&gt;PAPER&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;&#34;&gt;&lt;strong&gt;CODE&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;h1 id=&#34;abstrarct&#34;&gt;Abstrarct&lt;/h1&gt;
&lt;p&gt;We propose a novel convolutional operator for the task of point cloud completion. One striking characteristic of our approach is that, conversely to related work it does not require any max-pooling or voxelization operation. Instead, the proposed operator used to learn the point cloud embedding in the encoder extracts permutation-invariant features from the point cloud via a &lt;strong&gt;soft-pooling&lt;/strong&gt; of feature activations, which are able to preserve fine-grained geometric details. These features are then passed on to a decoder architecture. Due to the compression in the encoder, a typical limitation of this type of architectures is that they tend to lose parts of the input shape structure. We propose to overcome this limitation by using skip connections specifically devised for point clouds, where links between corresponding layers in the encoder and the decoder are established. As part of these connections, we introduce a transformation matrix that projects the features from the encoder to the decoder and vice-versa. The quantitative and qualitative results on the task of object completion from partial scans on the ShapeNet dataset show that our approach achieves state-of-the-art performance in shape completion both at low and high resolutions.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
