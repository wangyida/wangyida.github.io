<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Yida Wang</title><meta name=keywords content="Blog,Portfolio,PaperMod">
<meta name=description content="ExampleSite description">
<meta name=author content="Yida">
<link rel=canonical href=https://wangyida.github.io/>
<meta name=google-site-verification content="XYZabc">
<meta name=yandex-verification content="XYZabc">
<meta name=msvalidate.01 content="XYZabc">
<link crossorigin=anonymous href=/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css integrity="sha256-yIlj/i15RiAA/Q+xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as=style>
<link rel=preload href=/icon_Y.png as=image>
<link rel=icon href=https://wangyida.github.io/icon_Y.png>
<link rel=icon type=image/png sizes=16x16 href=https://wangyida.github.io/%3Clink%20/%20abs%20url%3E>
<link rel=icon type=image/png sizes=32x32 href=https://wangyida.github.io/icon_Y32x32.png>
<link rel=apple-touch-icon href=https://wangyida.github.io/icon_Y32x32.png>
<link rel=mask-icon href=https://wangyida.github.io/icon_Y32x32.png>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.93.1">
<link rel=alternate type=application/rss+xml href=https://wangyida.github.io/index.xml>
<noscript>
<style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,o,i,a,t,n,s){e.GoogleAnalyticsObject=t,e[t]=e[t]||function(){(e[t].q=e[t].q||[]).push(arguments)},e[t].l=1*new Date,n=o.createElement(i),s=o.getElementsByTagName(i)[0],n.async=1,n.src=a,s.parentNode.insertBefore(n,s)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-108746836-1","auto"),ga("send","pageview"))</script><meta property="og:title" content="Yida Wang">
<meta property="og:description" content="ExampleSite description">
<meta property="og:type" content="website">
<meta property="og:url" content="https://wangyida.github.io/"><meta property="og:image" content="http://campar.in.tum.de/twiki/pub/Main/YidaWang/YidaWang_CAMP.png"><meta property="og:site_name" content="ExampleSite">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="http://campar.in.tum.de/twiki/pub/Main/YidaWang/YidaWang_CAMP.png">
<meta name=twitter:title content="Yida Wang">
<meta name=twitter:description content="ExampleSite description">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","name":"Yida Wang","url":"https://wangyida.github.io/","description":"ExampleSite description","thumbnailUrl":"https://wangyida.github.io/icon_Y.png","sameAs":["https://www.linkedin.com/in/yida-wang-910123","https://scholar.google.com/citations?user=HpfFgYIAAAAJ\u0026hl=en","https://github.com/wangyida","https://www.youtube.com/channel/UCvu4OW3TufWFsj-vNrzeEiw","https://www.facebook.com/yida.wang.0123","https://wangyida.github.io/resume_yida_en.pdf"]}</script>
</head><body class=list id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://wangyida.github.io/ accesskey=h title=". WANG (Alt + H)">
<img src=/icon_Y.png alt=logo aria-label=logo height=35>. WANG</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div><ul id=menu>
<li>
<a href=https://wangyida.github.io/archives/ title=archive>
<span>archive</span>
</a>
</li><li>
<a href=https://wangyida.github.io/categories/ title=categories>
<span>categories</span>
</a>
</li><li>
<a href=https://wangyida.github.io/tags/ title=tags>
<span>tags</span>
</a>
</li><li>
<a href=http://campar.in.tum.de/Main/YidaWang title=Lab-site>
<span>Lab-site</span>
</a>
</li></ul></nav></header><main class=main>
<article class="first-entry home-info">
<header class=entry-header>
<h1>Yida WANG ✨</h1></header><section class=entry-content>
<p><p>I&rsquo;m a Ph.D. student focused on computer vision and machine learning from <a href=https://www.tum.de/>Technische Universität München</a>, with topic of deep learning techniques for representation and processing of 3D data. Projects have ever got sponsored by Microsoft, Google and Facebook Reality Labs, <em>e.g.</em> <a href=https://github.com/tiny-dnn/tiny-dnn>tiny-dnn</a> and <a href=https://github.com/opencv/opencv_contrib/tree/master/modules/cnn_3dobj>3D pose estimation</a>. <a href=/Presentation_Yida.pdf>Presentation</a></p><blockquote>
<ul>
<li><strong>Download</strong>: <a href=/resume_yida_en.pdf>My Resume</a>, <a href=/resume_yida_cn.pdf>中文简历</a> and <a href=/resume_yida_de.pdf>Lebenslauf</a></li></ul></blockquote></p></section><footer class=entry-footer>
<div class=social-icons>
<a href=https://www.linkedin.com/in/yida-wang-910123 target=_blank rel="noopener noreferrer me" title=Linkedin><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg>
</a>
<a href="https://scholar.google.com/citations?user=HpfFgYIAAAAJ&hl=en" target=_blank rel="noopener noreferrer me" title=Googlescholar><svg role="img" viewBox="0 0 24 25" xmlns="http://www.w3.org/2000/svg" fill="currentcolor" stroke="none" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M5.242 13.769.0 9.5 12 0l12 9.5-5.242 4.269C17.548 11.249 14.978 9.5 12 9.5c-2.977.0-5.548 1.748-6.758 4.269zM12 10a7 7 0 100 14 7 7 0 000-14z"/></svg>
</a>
<a href=https://github.com/wangyida target=_blank rel="noopener noreferrer me" title=Github><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg>
</a>
<a href=https://www.youtube.com/channel/UCvu4OW3TufWFsj-vNrzeEiw target=_blank rel="noopener noreferrer me" title=Youtube><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M22.54 6.42a2.78 2.78.0 00-1.94-2C18.88 4 12 4 12 4s-6.88.0-8.6.46a2.78 2.78.0 00-1.94 2A29 29 0 001 11.75a29 29 0 00.46 5.33A2.78 2.78.0 003.4 19c1.72.46 8.6.46 8.6.46s6.88.0 8.6-.46a2.78 2.78.0 001.94-2 29 29 0 00.46-5.25 29 29 0 00-.46-5.33z"/><polygon points="9.75 15.02 15.5 11.75 9.75 8.48 9.75 15.02"/></svg>
</a>
<a href=https://www.facebook.com/yida.wang.0123 target=_blank rel="noopener noreferrer me" title=Facebook><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M18 2h-3a5 5 0 00-5 5v3H7v4h3v8h4v-8h3l1-4h-4V7a1 1 0 011-1h3z"/></svg>
</a>
<a href=https://wangyida.github.io/resume_yida_en.pdf target=_blank rel="noopener noreferrer me" title=Cv><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4v16a2 2 0 002 2h12a2 2 0 002-2V8.342a2 2 0 00-.602-1.43l-4.44-4.342A2 2 0 0013.56 2H6A2 2 0 004 4z"/><path d="M9 13h6"/><path d="M9 17h3"/><path d="M14 2v4a2 2 0 002 2h4"/></svg>
</a>
</div></footer></article><article class=post-entry>
<figure class=entry-cover><img loading=lazy src=https://wangyida.github.io/teasers/thesis_teaser_pami22.png alt="caption for image">
</figure><header class=entry-header>
<h2>Self-supervised Latent Space Optimization with Nebula Variational Coding
</h2></header><section class=entry-content>
<p>| paper | code |
Abstrarct Deep learning approaches process data in a layer-by-layer way with intermediate (or latent) features. We aim at designing a general solution to optimize the latent manifolds to improve the performance on classification, segmentation, completion and/or reconstruction through probabilistic models. This paper proposes a variational inference model which leads to a clustered embedding. We introduce additional variables in the latent space, called nebula anchors, that guide the latent variables to form clusters during training....</p></section><footer class=entry-footer><span title="2022-03-01 10:15:01 +0200 +0200">March 1, 2022</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;Yida</footer><a class=entry-link aria-label="post link to Self-supervised Latent Space Optimization with Nebula Variational Coding" href=https://wangyida.github.io/posts/2022/youtube/></a>
</article><article class=post-entry>
<figure class=entry-cover><img loading=lazy src=https://wangyida.github.io/teasers/thesis_teaser_cvpr22.png alt="caption for image">
</figure><header class=entry-header>
<h2>Learning Local Displacements for Point Cloud Completion
</h2></header><section class=entry-content>
<p>| paper | code |
Abstrarct We propose a novel approach aimed at object and semantic scene completion from a partial scan represented as a 3D point cloud. Our architecture relies on three novel layers that are used successively within an encoder-decoder structure and specifically developed for the task at hand. The first one carries out feature extraction by matching the point features to a set of pre-trained local descriptors. Then, to avoid losing individual descriptors as part of standard operations such as max-pooling, we propose an alternative neighbor-pooling operation that relies on adopting the feature vectors with the highest activations....</p></section><footer class=entry-footer><span title="2022-02-19 10:15:01 +0200 +0200">February 19, 2022</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;Yida</footer><a class=entry-link aria-label="post link to Learning Local Displacements for Point Cloud Completion" href=https://wangyida.github.io/posts/2022_1/youtube/></a>
</article><article class=post-entry>
<figure class=entry-cover><img loading=lazy src=https://wangyida.github.io/teasers/thesis_teaser_ijcv22.png alt="caption for image">
</figure><header class=entry-header>
<h2>SoftPool++ / An Encoder-Decoder Network for Point Cloud Completion
</h2></header><section class=entry-content>
<p>| paper | code |
Abstrarct We propose a novel convolutional operator for the task of point cloud completion. One striking characteristic of our approach is that, conversely to related work it does not require any max-pooling or voxelization operation. Instead, the proposed operator used to learn the point cloud embedding in the encoder extracts permutation-invariant features from the point cloud via a soft-pooling of feature activations, which are able to preserve fine-grained geometric details....</p></section><footer class=entry-footer><span title="2021-12-29 10:15:01 +0200 +0200">December 29, 2021</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;Yida</footer><a class=entry-link aria-label="post link to SoftPool++ / An Encoder-Decoder Network for Point Cloud Completion" href=https://wangyida.github.io/posts/2021/youtube/></a>
</article><article class=post-entry>
<figure class=entry-cover><img loading=lazy src=https://wangyida.github.io/teasers/thesis_teaser_eccv20.png alt="caption for image">
</figure><header class=entry-header>
<h2>SoftPoolNet - Shape Descriptor for Point Cloud Completion and Classification
</h2></header><section class=entry-content>
<p>| paper | code |
Abstrarct Point clouds are often the default choice for many applications as they exhibit more flexibility and efficiency than volumetric data. Nevertheless, their unorganized nature – points are stored in an unordered way – makes them less suited to be processed by deep learning pipelines. In this paper, we propose a method for 3D object completion and classification based on point clouds. We introduce a new way of organizing the extracted features based on their activations, which we name soft pooling....</p></section><footer class=entry-footer><span title="2020-08-25 10:15:01 +0200 CEST">August 25, 2020</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;Yida</footer><a class=entry-link aria-label="post link to SoftPoolNet - Shape Descriptor for Point Cloud Completion and Classification" href=https://wangyida.github.io/posts/2020/youtube/></a>
</article><article class=post-entry>
<figure class=entry-cover><img loading=lazy src=https://wangyida.github.io/teasers/thesis_teaser_iccv19.png alt="caption for image">
</figure><header class=entry-header>
<h2>ForkNet - Multi-Branch Volumetric Semantic Completion From a Single Depth Image
</h2></header><section class=entry-content>
<p>| paper | code |
Abstrarct We propose a novel model for 3D semantic completion from a single depth image, based on a single encoder and three separate generators used to reconstruct different geometric and semantic representations of the original and completed scene, all sharing the same latent space. To transfer information between the geometric and semantic branches of the network, we introduce paths between them concatenating features at corresponding network layers....</p></section><footer class=entry-footer><span title="2019-11-01 10:15:01 +0200 +0200">November 1, 2019</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;Yida</footer><a class=entry-link aria-label="post link to ForkNet - Multi-Branch Volumetric Semantic Completion From a Single Depth Image" href=https://wangyida.github.io/posts/2019/youtube/></a>
</article><article class=post-entry>
<figure class=entry-cover><img loading=lazy src=https://wangyida.github.io/teasers/thesis_teaser_ral19.png alt="caption for image">
</figure><header class=entry-header>
<h2>Variational Object-aware 3D Hand Pose from a Single RGB Image
</h2></header><section class=entry-content>
<p>| paper | code |
Abstrarct We propose an approach to estimate the 3D pose of a human hand while grasping objects from a single RGB image. Our approach is based on a probabilistic model implemented with deep architectures, which is used for regressing, respectively, the 2D hand joints heat maps and the 3D hand joints coordinates. We train our networks so to make our approach robust to large object- and self-occlusions, as commonly occurring with the task at hand....</p></section><footer class=entry-footer><span title="2019-06-01 10:15:01 +0200 CEST">June 1, 2019</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;Yida</footer><a class=entry-link aria-label="post link to Variational Object-aware 3D Hand Pose from a Single RGB Image" href=https://wangyida.github.io/posts/2019_1/youtube/></a>
</article><article class=post-entry>
<figure class=entry-cover><img loading=lazy src=https://wangyida.github.io/teasers/thesis_teaser_3dv18.png alt="caption for image">
</figure><header class=entry-header>
<h2>Adversarial Semantic Scene Completion from a Single Depth Image
</h2></header><section class=entry-content>
<p>| paper | code |
Abstrarct We propose a method to reconstruct, complete and semantically label a 3D scene from a single input depth image. We improve the accuracy of the regressed semantic 3D maps by a novel architecture based on adversarial learning. In particular, we suggest using multiple adversarial loss terms that not only enforce realistic outputs with respect to the ground truth, but also an effective embedding of the internal features....</p></section><footer class=entry-footer><span title="2018-10-09 10:15:01 +0200 CEST">October 9, 2018</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;Yida</footer><a class=entry-link aria-label="post link to Adversarial Semantic Scene Completion from a Single Depth Image" href=https://wangyida.github.io/posts/2018/youtube/></a>
</article><article class=post-entry>
<header class=entry-header>
<h2>Generative Model with Coordinate Metric Learning for Object Recognition Based on 3D Models
</h2></header><section class=entry-content>
<p>Abstrarct Collecting data for deep learning is so tedious which makes it hard to establish a perfect database. In this paper, we propose a generative model trained with synthetic images rendered from 3D models which can reduce the burden on collecting real training data and make the background conditions more sundry. Our architecture is composed of two subnetworks: semantic foreground object reconstruction network based on Bayesian inference and classification network based on multi-triplet cost training for avoiding over-fitting on monotone synthetic object surface and utilizing accurate informations of synthetic images like object poses and lightning conditions which are helpful for recognizing regular photos....</p></section><footer class=entry-footer><span title="2017-11-15 10:15:01 +0200 +0200">November 15, 2017</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;Yida</footer><a class=entry-link aria-label="post link to Generative Model with Coordinate Metric Learning for Object Recognition Based on 3D Models" href=https://wangyida.github.io/posts/2017/youtube/></a>
</article><footer class=page-footer>
<nav class=pagination>
<a class=next href=https://wangyida.github.io/page/2/>Next Page »</a>
</nav></footer></main><footer class=footer>
<span>&copy; 2022 <a href=https://wangyida.github.io/>Yida Wang</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script>
</body></html>